# Biomni Custom Model Configuration
# Use this profile for custom model serving (Ollama, vLLM, SGLang, etc.)

# Custom model serving configuration
# Set your custom model base URL (e.g., Ollama, vLLM, SGLang)
CUSTOM_MODEL_BASE_URL=http://localhost:8000/v1

# Optional: Custom API key (if your server requires it)
CUSTOM_MODEL_API_KEY=your_custom_api_key_here

# Set source to Custom
LLM_SOURCE=Custom

# Set your custom model name
# Examples:
# - Ollama: llama3.2, mistral, qwen2.5
# - vLLM/SGLang: meta-llama/Llama-3.2-3B-Instruct, biomni/Biomni-R0-32B-Preview
BIOMNI_LLM=your_model_name_here

# Optional: Temperature for generation
# BIOMNI_TEMPERATURE=0.7

# Optional: Timeout settings
# BIOMNI_TIMEOUT_SECONDS=600

# Optional: Data path
# BIOMNI_DATA_PATH=./data

# Note: When using custom models, make sure your model server is running
# Example commands to start model servers:
#
# Ollama:
#   ollama serve
#
# vLLM:
#   python -m vllm.entrypoints.openai.api_server --model modelName --port 8000
#
# SGLang (for Biomni-R0):
#   python -m sglang.launch_server --model-path RyanLi0802/Biomni-R0-Preview \
#     --port 30000 --host 0.0.0.0 --mem-fraction-static 0.8 --tp 2 \
#     --trust-remote-code --json-model-override-args \
#     '{"rope_scaling":{"rope_type":"yarn","factor":1.0, \
#       "original_max_position_embeddings":32768}, \
#       "max_position_embeddings": 131072}'
